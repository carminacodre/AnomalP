{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE on proteins in SA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "from keras import Input\n",
    "from keras.layers import Dense, Lambda, Conv1D\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy, mse\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment on a subset of families, test different model capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = [\"fam_1\", \"fam_2\", \"fam_3\"]\n",
    "ds_path = \"Dataset/families_reduced\"\n",
    "family_paths = {}\n",
    "for f in families:\n",
    "    family_paths[f]= os.path.join(ds_path, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_family(f):\n",
    "    proteins = glob.glob(os.path.join(family_paths[f], \"*.out\"))\n",
    "    print(\"Proteins for family %s\" %f)\n",
    "    for p in proteins:\n",
    "        print(p)\n",
    "    proteins_conf = []\n",
    "    for p in proteins:\n",
    "        with open(p) as in_file:\n",
    "            for line in in_file:\n",
    "                proteins_conf.append(line.strip())\n",
    "    len(proteins_conf)\n",
    "    l = [len(p) for p in proteins_conf]\n",
    "    print(Counter(l))\n",
    "    return proteins_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families_conf = {}\n",
    "for f in families:\n",
    "    families_conf[f] = load_family(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_di=dict(zip(string.ascii_letters,[ord(c)%32 for c in string.ascii_letters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = True\n",
    "num_classes = 27 if padding else 26 # 0 is left for padding\n",
    "categorical = True\n",
    "normalize = False\n",
    "max_length = 144\n",
    "flatten = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conf(configurations, categorical, padding, max_length, normalize, flatten):\n",
    "    proteins_processed = [[letters_di[l] for l in p] for p in configurations]\n",
    "    if padding:\n",
    "        # pad sequences if less than max length\n",
    "        proteins_processed = [p if len(p) == max_length else p + [0] * (max_length - len(p)) for p in proteins_processed]\n",
    "    # tranforms data to one hot encodings\n",
    "    if categorical:\n",
    "        proteins_processed = [to_categorical(p, num_classes=num_classes) for p in proteins_processed]   \n",
    "    proteins_processed = np.array([np.array(x) for x in proteins_processed])\n",
    "    if flatten:\n",
    "        proteins_processed = proteins_processed.reshape(-1, num_classes * max_length)\n",
    "    if normalize:\n",
    "        proteins_processed = proteins_processed.astype('float32') / (letters_di['Z'] * 1.0)\n",
    "    return proteins_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in families:\n",
    "    families_conf[f] = process_conf(families_conf[f], categorical=categorical, padding=padding, max_length=max_length,normalize=normalize, flatten=flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families_conf['fam_1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "intermediate_dim = 2\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dirs\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")\n",
    "if not os.path.exists(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "for f in families:\n",
    "    if not os.path.exists(os.path.join(\"models\", f)):\n",
    "        os.mkdir(os.path.join(\"models\", f))\n",
    "    if not os.path.exists(os.path.join(\"logs\", f)):\n",
    "        os.mkdir(os.path.join(\"logs\", f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoints(f):\n",
    "    checkpoints_path = os.path.join(\"models\", f)\n",
    "    tensorboard_path = os.path.join(\"logs\", f)\n",
    "    cp_cb = ModelCheckpoint(filepath=os.path.join(checkpoints_path, \"model.{epoch:02d}.hdf5\"), monitor='val_loss',\n",
    "                            save_best_only=True)\n",
    "    tb_cb = TensorBoard(log_dir=tensorboard_path)\n",
    "    return [cp_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder\n",
    "def get_ae():\n",
    "    if categorical:\n",
    "        if not flatten:\n",
    "            model_input = Input(shape=(None,num_classes))\n",
    "        else:\n",
    "            model_input = Input(shape=(max_length*num_classes,))\n",
    "    else:\n",
    "        model_input = Input(shape=(max_length,))\n",
    "    #x=Conv1D(intermediate_dim, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1)(model_input)\n",
    "    #encoded=Conv1D(intermediate_dim, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1, name=\"encoded\")(x)\n",
    "    #x=Conv1D(num_classes, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1)(encoded)\n",
    "    encoded= Dense(intermediate_dim, activation='sigmoid')(model_input)\n",
    "    if categorical:\n",
    "        if not flatten:\n",
    "            x = Dense(num_classes, activation='sigmoid')(encoded)\n",
    "        else:\n",
    "            x = Dense(max_length*num_classes, activation='sigmoid')(encoded)\n",
    "    else:\n",
    "        x = Dense(max_length, activation='sigmoid')(encoded)\n",
    "    ae=Model(inputs=model_input, outputs=[x])\n",
    "    opt=RMSprop(lr=0.01)\n",
    "    ae.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "    ae.summary()\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "\n",
    "Using convolutional layers shows poor convergence.\n",
    "\n",
    "Itermediate dimension reduced to 2 seems to work for 3 families, but not for more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the autoencoder for specific classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = {}\n",
    "train_data = {}\n",
    "for f in families:\n",
    "    train, test = train_test_split(families_conf[f], test_size=0.25, random_state=42)\n",
    "    test_data[f] = test\n",
    "    train_data[f] = train\n",
    "    print(train.shape)\n",
    "    print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in families: \n",
    "    print(\"Training for family %s\" %f)\n",
    "    ae = get_ae()\n",
    "    ae.fit(train_data[f], train_data[f],\n",
    "           shuffle=True,\n",
    "           epochs=epochs,\n",
    "           batch_size=batch_size,\n",
    "           validation_data=(test_data[f], test_data[f]),\n",
    "           callbacks=create_checkpoints(f),\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"fam_1\": \"models/fam_1/model.28.hdf5\",\n",
    "          \"fam_2\": \"models/fam_2/model.30.hdf5\",\n",
    "          \"fam_3\": \"models/fam_3/model.30.hdf5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_fam(f):\n",
    "    ae = load_model(models[f])\n",
    "    ae.summary()\n",
    "    losses_train = []\n",
    "    for t in train_data[f]:\n",
    "        losses_train.append(ae.evaluate(np.array([t]),np.array([t]), verbose=0))\n",
    "    max_l = max(losses_train)\n",
    "    losses_test = []\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    for ft in families: \n",
    "        for t in test_data[ft]:\n",
    "            loss=ae.evaluate(np.array([t]),np.array([t]), verbose=0)\n",
    "            if loss > max_l:\n",
    "                # predict other family\n",
    "                if ft == f:\n",
    "                    fn+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "            else:\n",
    "                # predict current family\n",
    "                if ft == f:\n",
    "                   tp+=1 \n",
    "                else:\n",
    "                    fp+=1\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = evaluate_for_fam(\"fam_1\")\n",
    "print(\"True positives  %d\" %tp)\n",
    "print(\"True negatives  %d\" %tn)\n",
    "print(\"False positives %d\" %fp)\n",
    "print(\"False negatives %d\" %fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = evaluate_for_fam(\"fam_2\")\n",
    "print(\"True positives  %d\" %tp)\n",
    "print(\"True negatives  %d\" %tn)\n",
    "print(\"False positives %d\" %fp)\n",
    "print(\"False negatives %d\" %fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, tn, fp, fn = evaluate_for_fam(\"fam_3\")\n",
    "print(\"True positives  %d\" %tp)\n",
    "print(\"True negatives  %d\" %tn)\n",
    "print(\"False positives %d\" %fp)\n",
    "print(\"False negatives %d\" %fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dizy",
   "language": "python",
   "name": "dizy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
