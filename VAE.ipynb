{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE on proteins in SA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "from keras import Input\n",
    "from keras.layers import Dense, Lambda\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy, mse\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_path = \"Dataset/families_reduced/fam_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset/families_reduced/fam_1/1DLW.lf_str.out\n",
      "Dataset/families_reduced/fam_1/1ECA.lf_str.out\n",
      "Dataset/families_reduced/fam_1/1ASH.lf_str.out\n"
     ]
    }
   ],
   "source": [
    "proteins = glob.glob(os.path.join(family_path, \"*.out\"))\n",
    "for p in proteins:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proteins_conf = []\n",
    "for p in proteins:\n",
    "    with open(p) as in_file:\n",
    "        for line in in_file:\n",
    "            proteins_conf.append(line.strip())\n",
    "len(proteins_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({113: 10000, 133: 10000, 144: 10000})\n"
     ]
    }
   ],
   "source": [
    "l = [len(p) for p in proteins_conf]\n",
    "print(Counter(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 144\n",
    "num_classes = 27 # including padding 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "letters_di=dict(zip(string.ascii_letters,[ord(c)%32 for c in string.ascii_letters]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trasnform letters to integers\n",
    "proteins_processed = [[letters_di[l] for l in p] for p in proteins_conf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sequences if less than max length\n",
    "proteins_processed = [p if len(p) == max_length else p + [0] * (max_length - len(p)) for p in proteins_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tranforms data to one hot encodings\n",
    "proteins_processed = [to_categorical(p, num_classes=num_classes) for p in proteins_processed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(proteins_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = proteins_processed[:(len(proteins_processed)*75//100)]\n",
    "test_set = proteins_processed[(len(proteins_processed)*75//100):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np = np.array([np.array(x) for x in train_set])\n",
    "test_np = np.array([np.array(x) for x in test_set])\n",
    "train_np = train_np.astype('float32') / letters_di['Z'] * 1.0\n",
    "test_np = test_np.astype('float32') / letters_di['Z'] * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22500, 144, 27)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7500, 144, 27)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "original_dim = 144 * 27\n",
    "intermediate_dim = 300\n",
    "latent_dim = 2\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_np = np.reshape(train_np, [-1, original_dim])\n",
    "test_np = np.reshape(test_np,[-1, original_dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/carminacodre/.virtualenvs/dizy/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#encoder\n",
    "inputs = Input(shape=(original_dim,), name='encoder_input')\n",
    "x = Dense(intermediate_dim, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_dim, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_dim, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    \"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "    # Arguments\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean = 0 and std = 1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 3888)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 300)          1166700     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            602         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            602         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 1,167,904\n",
      "Trainable params: 1,167,904\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder\n",
    "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
    "x = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
    "outputs = Dense(original_dim, activation='sigmoid')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "z_sampling (InputLayer)      (None, 2)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 300)               900       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3888)              1170288   \n",
      "=================================================================\n",
      "Total params: 1,171,188\n",
      "Trainable params: 1,171,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 3888)              0         \n",
      "_________________________________________________________________\n",
      "encoder (Model)              [(None, 2), (None, 2), (N 1167904   \n",
      "_________________________________________________________________\n",
      "decoder (Model)              (None, 3888)              1171188   \n",
      "=================================================================\n",
      "Total params: 2,339,092\n",
      "Trainable params: 2,339,092\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "reconstruction_loss = binary_crossentropy(inputs,outputs)\n",
    "reconstruction_loss *= original_dim\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "opt = Adam(lr=0.001)\n",
    "vae.compile(optimizer=opt)\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/carminacodre/.virtualenvs/dizy/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 22500 samples, validate on 7500 samples\n",
      "Epoch 1/100\n",
      "22500/22500 [==============================] - 12s 522us/step - loss: 269.6189 - val_loss: 47.9163\n",
      "Epoch 2/100\n",
      "22500/22500 [==============================] - 10s 458us/step - loss: 39.2274 - val_loss: 35.8777\n",
      "Epoch 3/100\n",
      "22500/22500 [==============================] - 11s 472us/step - loss: 34.8840 - val_loss: 34.0938\n",
      "Epoch 4/100\n",
      "22500/22500 [==============================] - 11s 471us/step - loss: 33.6689 - val_loss: 33.2817\n",
      "Epoch 5/100\n",
      "22500/22500 [==============================] - 10s 456us/step - loss: 32.9147 - val_loss: 32.3821\n",
      "Epoch 6/100\n",
      "22500/22500 [==============================] - 10s 451us/step - loss: 31.8191 - val_loss: 31.4332\n",
      "Epoch 7/100\n",
      "22500/22500 [==============================] - 10s 448us/step - loss: 31.1183 - val_loss: 30.6894\n",
      "Epoch 8/100\n",
      "22500/22500 [==============================] - 10s 449us/step - loss: 30.1794 - val_loss: 29.8009\n",
      "Epoch 9/100\n",
      "22500/22500 [==============================] - 10s 446us/step - loss: 29.5426 - val_loss: 29.3043\n",
      "Epoch 10/100\n",
      "22500/22500 [==============================] - 10s 449us/step - loss: 29.0897 - val_loss: 28.8721\n",
      "Epoch 11/100\n",
      "22500/22500 [==============================] - 10s 451us/step - loss: 28.7087 - val_loss: 28.5672\n",
      "Epoch 12/100\n",
      "22500/22500 [==============================] - 10s 462us/step - loss: 28.4397 - val_loss: 28.2969\n",
      "Epoch 13/100\n",
      "22500/22500 [==============================] - 10s 455us/step - loss: 28.2407 - val_loss: 28.1806\n",
      "Epoch 14/100\n",
      "22500/22500 [==============================] - 10s 458us/step - loss: 28.1342 - val_loss: 28.1139\n",
      "Epoch 15/100\n",
      "22500/22500 [==============================] - 10s 454us/step - loss: 28.0565 - val_loss: 28.0378\n",
      "Epoch 16/100\n",
      "22500/22500 [==============================] - 10s 463us/step - loss: 28.0271 - val_loss: 28.0120\n",
      "Epoch 17/100\n",
      "22500/22500 [==============================] - 10s 456us/step - loss: 27.9919 - val_loss: 27.9885\n",
      "Epoch 18/100\n",
      "22500/22500 [==============================] - 10s 451us/step - loss: 27.9554 - val_loss: 27.9383\n",
      "Epoch 19/100\n",
      "22500/22500 [==============================] - 10s 455us/step - loss: 27.9265 - val_loss: 27.9580\n",
      "Epoch 20/100\n",
      "22500/22500 [==============================] - 10s 455us/step - loss: 27.9177 - val_loss: 27.8823\n",
      "Epoch 21/100\n",
      "22500/22500 [==============================] - 10s 454us/step - loss: 27.8997 - val_loss: 27.8903\n",
      "Epoch 22/100\n",
      "22500/22500 [==============================] - 10s 451us/step - loss: 27.8857 - val_loss: 27.8787\n",
      "Epoch 23/100\n",
      "22500/22500 [==============================] - 10s 456us/step - loss: 27.8748 - val_loss: 27.8500\n",
      "Epoch 24/100\n",
      "22500/22500 [==============================] - 10s 463us/step - loss: 27.8635 - val_loss: 27.8724\n",
      "Epoch 25/100\n",
      "22500/22500 [==============================] - 10s 455us/step - loss: 27.8552 - val_loss: 27.8364\n",
      "Epoch 26/100\n",
      "22500/22500 [==============================] - 10s 456us/step - loss: 27.8346 - val_loss: 27.8360\n",
      "Epoch 27/100\n",
      "22500/22500 [==============================] - 10s 458us/step - loss: 27.8199 - val_loss: 27.8453\n",
      "Epoch 28/100\n",
      "22500/22500 [==============================] - 12s 540us/step - loss: 27.8317 - val_loss: 27.8375\n",
      "Epoch 29/100\n",
      "22500/22500 [==============================] - 15s 662us/step - loss: 27.8140 - val_loss: 27.8161\n",
      "Epoch 30/100\n",
      "22500/22500 [==============================] - 13s 592us/step - loss: 27.8175 - val_loss: 27.8060\n",
      "Epoch 31/100\n",
      "22500/22500 [==============================] - 12s 516us/step - loss: 27.7937 - val_loss: 27.8110\n",
      "Epoch 32/100\n",
      "22500/22500 [==============================] - 13s 573us/step - loss: 27.8006 - val_loss: 27.8284\n",
      "Epoch 33/100\n",
      "22500/22500 [==============================] - 12s 539us/step - loss: 27.7996 - val_loss: 27.8140\n",
      "Epoch 34/100\n",
      "22500/22500 [==============================] - 10s 466us/step - loss: 27.7929 - val_loss: 27.8004\n",
      "Epoch 35/100\n",
      "22500/22500 [==============================] - 10s 454us/step - loss: 27.7864 - val_loss: 27.7766\n",
      "Epoch 36/100\n",
      "22500/22500 [==============================] - 10s 453us/step - loss: 27.7808 - val_loss: 27.8079\n",
      "Epoch 37/100\n",
      "22500/22500 [==============================] - 11s 482us/step - loss: 27.7611 - val_loss: 27.7518\n",
      "Epoch 38/100\n",
      "22500/22500 [==============================] - 10s 447us/step - loss: 27.7670 - val_loss: 27.7659\n",
      "Epoch 39/100\n",
      "22500/22500 [==============================] - 10s 453us/step - loss: 27.7703 - val_loss: 27.7819\n",
      "Epoch 40/100\n",
      "22500/22500 [==============================] - 10s 447us/step - loss: 27.7592 - val_loss: 27.7512\n",
      "Epoch 41/100\n",
      "22500/22500 [==============================] - 10s 448us/step - loss: 27.7756 - val_loss: 27.7404\n",
      "Epoch 42/100\n",
      "22500/22500 [==============================] - 10s 451us/step - loss: 27.7588 - val_loss: 27.7661\n",
      "Epoch 43/100\n",
      "22500/22500 [==============================] - 11s 467us/step - loss: 27.7500 - val_loss: 27.7934\n",
      "Epoch 44/100\n",
      "22500/22500 [==============================] - 11s 488us/step - loss: 27.7503 - val_loss: 27.7634\n",
      "Epoch 45/100\n",
      "22500/22500 [==============================] - 11s 498us/step - loss: 27.7490 - val_loss: 27.7373\n",
      "Epoch 46/100\n",
      "22500/22500 [==============================] - 11s 508us/step - loss: 27.7729 - val_loss: 27.7364\n",
      "Epoch 47/100\n",
      "22500/22500 [==============================] - 12s 535us/step - loss: 27.7524 - val_loss: 27.7481\n",
      "Epoch 48/100\n",
      "22500/22500 [==============================] - 12s 545us/step - loss: 27.7470 - val_loss: 27.7079\n",
      "Epoch 49/100\n",
      "22500/22500 [==============================] - 11s 488us/step - loss: 27.7388 - val_loss: 27.7476\n",
      "Epoch 50/100\n",
      "22500/22500 [==============================] - 11s 486us/step - loss: 27.7479 - val_loss: 27.7577\n",
      "Epoch 51/100\n",
      "22500/22500 [==============================] - 11s 501us/step - loss: 27.7491 - val_loss: 27.7428\n",
      "Epoch 52/100\n",
      "22500/22500 [==============================] - 11s 493us/step - loss: 27.7394 - val_loss: 27.7510\n",
      "Epoch 53/100\n",
      "22500/22500 [==============================] - 11s 495us/step - loss: 27.7296 - val_loss: 27.7548\n",
      "Epoch 54/100\n",
      "22500/22500 [==============================] - 11s 470us/step - loss: 27.7405 - val_loss: 27.7784\n",
      "Epoch 55/100\n",
      "22500/22500 [==============================] - 11s 489us/step - loss: 27.7336 - val_loss: 27.7228\n",
      "Epoch 56/100\n",
      "22500/22500 [==============================] - 10s 466us/step - loss: 27.7392 - val_loss: 27.7274\n",
      "Epoch 57/100\n",
      "22500/22500 [==============================] - 12s 513us/step - loss: 27.7370 - val_loss: 27.7576\n",
      "Epoch 58/100\n",
      "22500/22500 [==============================] - 11s 495us/step - loss: 27.7230 - val_loss: 27.7191\n",
      "Epoch 59/100\n",
      "22500/22500 [==============================] - 13s 564us/step - loss: 27.7174 - val_loss: 27.7184\n",
      "Epoch 60/100\n",
      "22500/22500 [==============================] - 10s 455us/step - loss: 27.7221 - val_loss: 27.7403\n",
      "Epoch 61/100\n",
      "22500/22500 [==============================] - 10s 463us/step - loss: 27.7365 - val_loss: 27.7177\n",
      "Epoch 62/100\n",
      "22500/22500 [==============================] - 10s 453us/step - loss: 27.7231 - val_loss: 27.7455\n",
      "Epoch 63/100\n",
      "22500/22500 [==============================] - 10s 453us/step - loss: 27.7333 - val_loss: 27.7368\n",
      "Epoch 64/100\n",
      "22500/22500 [==============================] - 10s 454us/step - loss: 27.7360 - val_loss: 27.7566\n",
      "Epoch 65/100\n",
      "22500/22500 [==============================] - 10s 453us/step - loss: 27.7232 - val_loss: 27.7237\n",
      "Epoch 66/100\n",
      "22500/22500 [==============================] - 11s 468us/step - loss: 27.7079 - val_loss: 27.7265\n",
      "Epoch 67/100\n",
      "22500/22500 [==============================] - 10s 449us/step - loss: 27.7335 - val_loss: 27.7167\n",
      "Epoch 68/100\n",
      "22500/22500 [==============================] - 10s 466us/step - loss: 27.7168 - val_loss: 27.7434\n",
      "Epoch 69/100\n",
      "22500/22500 [==============================] - 11s 481us/step - loss: 27.7231 - val_loss: 27.7104\n",
      "Epoch 70/100\n",
      "22500/22500 [==============================] - 10s 448us/step - loss: 27.7292 - val_loss: 27.7242\n",
      "Epoch 71/100\n",
      "22500/22500 [==============================] - 10s 444us/step - loss: 27.7098 - val_loss: 27.7412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "22500/22500 [==============================] - 10s 429us/step - loss: 27.7172 - val_loss: 27.6984\n",
      "Epoch 73/100\n",
      "22500/22500 [==============================] - 10s 427us/step - loss: 27.7223 - val_loss: 27.7338\n",
      "Epoch 74/100\n",
      "22500/22500 [==============================] - 10s 428us/step - loss: 27.7077 - val_loss: 27.7172\n",
      "Epoch 75/100\n",
      "22500/22500 [==============================] - 10s 429us/step - loss: 27.7176 - val_loss: 27.7180\n",
      "Epoch 76/100\n",
      "22500/22500 [==============================] - 10s 425us/step - loss: 27.7277 - val_loss: 27.6865\n",
      "Epoch 77/100\n",
      "22500/22500 [==============================] - 10s 423us/step - loss: 27.7324 - val_loss: 27.6953\n",
      "Epoch 78/100\n",
      "22500/22500 [==============================] - 10s 423us/step - loss: 27.7214 - val_loss: 27.7156\n",
      "Epoch 79/100\n",
      "22500/22500 [==============================] - 10s 424us/step - loss: 27.7245 - val_loss: 27.7078\n",
      "Epoch 80/100\n",
      "22500/22500 [==============================] - 10s 425us/step - loss: 27.7144 - val_loss: 27.6984\n",
      "Epoch 81/100\n",
      "22500/22500 [==============================] - 10s 426us/step - loss: 27.7091 - val_loss: 27.6968\n",
      "Epoch 82/100\n",
      "22500/22500 [==============================] - 10s 428us/step - loss: 27.7219 - val_loss: 27.7205\n",
      "Epoch 83/100\n",
      "22500/22500 [==============================] - 10s 425us/step - loss: 27.7052 - val_loss: 27.6972\n",
      "Epoch 84/100\n",
      "22500/22500 [==============================] - 10s 424us/step - loss: 27.7120 - val_loss: 27.7124\n",
      "Epoch 85/100\n",
      "22500/22500 [==============================] - 10s 424us/step - loss: 27.7041 - val_loss: 27.7103\n",
      "Epoch 86/100\n",
      "22500/22500 [==============================] - 10s 425us/step - loss: 27.7152 - val_loss: 27.6916\n",
      "Epoch 87/100\n",
      "22500/22500 [==============================] - 10s 424us/step - loss: 27.7116 - val_loss: 27.7023\n",
      "Epoch 88/100\n",
      "22500/22500 [==============================] - 10s 424us/step - loss: 27.7043 - val_loss: 27.6958\n",
      "Epoch 89/100\n",
      "22500/22500 [==============================] - 10s 424us/step - loss: 27.7074 - val_loss: 27.6996\n",
      "Epoch 90/100\n",
      "22500/22500 [==============================] - 10s 433us/step - loss: 27.7118 - val_loss: 27.6984\n",
      "Epoch 91/100\n",
      "22500/22500 [==============================] - 10s 461us/step - loss: 27.7042 - val_loss: 27.7035\n",
      "Epoch 92/100\n",
      "22500/22500 [==============================] - 10s 448us/step - loss: 27.7157 - val_loss: 27.7110\n",
      "Epoch 93/100\n",
      "22500/22500 [==============================] - 10s 440us/step - loss: 27.7159 - val_loss: 27.7512\n",
      "Epoch 94/100\n",
      "22500/22500 [==============================] - 10s 443us/step - loss: 27.7085 - val_loss: 27.7181\n",
      "Epoch 95/100\n",
      "22500/22500 [==============================] - 10s 443us/step - loss: 27.7088 - val_loss: 27.7033\n",
      "Epoch 96/100\n",
      "22500/22500 [==============================] - 10s 446us/step - loss: 27.7028 - val_loss: 27.6913\n",
      "Epoch 97/100\n",
      "22500/22500 [==============================] - 10s 445us/step - loss: 27.7117 - val_loss: 27.6989\n",
      "Epoch 98/100\n",
      "22500/22500 [==============================] - 10s 447us/step - loss: 27.7067 - val_loss: 27.6970\n",
      "Epoch 99/100\n",
      "22500/22500 [==============================] - 10s 445us/step - loss: 27.7077 - val_loss: 27.6790\n",
      "Epoch 100/100\n",
      "22500/22500 [==============================] - 10s 444us/step - loss: 27.7229 - val_loss: 27.6850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d7a3c18>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.fit(x=train_np,\n",
    "        shuffle=True,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(test_np, None),\n",
    "        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = vae.predict(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 20, 21, 21, 21, 21, 21, 22, 23, 20, 18, 23, 23, 18,  8,  5, 11,\n",
       "       22, 15, 13, 22, 22, 21, 21, 22, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
       "       23, 14, 13, 22, 20, 22, 20, 22, 22, 13, 22, 23, 17, 23, 17,  8, 12,\n",
       "       21, 22, 19, 22, 22, 14, 10, 20, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 22, 23, 13, 22, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 23, 16, 17,  5,  2,\n",
       "        9, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 19,\n",
       "       24, 20, 14, 11,  5,  5, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 22, 21, 21, 21, 23])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.reshape(test_np[0],[-1,27]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 20, 21, 21, 21, 21, 21, 21, 21, 22, 19, 23, 23, 18,  8,  5, 11,\n",
       "       18, 10, 19, 22, 22, 23, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       23, 14, 13, 20, 20, 22, 22, 21, 22, 13, 22, 23, 17, 23, 17,  8, 11,\n",
       "       21, 23, 19, 22, 22, 14, 10, 20, 21, 21, 21, 23, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 20, 20, 23, 12, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 23, 16, 13,  5,  2,\n",
       "        9, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22,\n",
       "       21, 23, 14, 11, 13,  4, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 23])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.reshape(test_res[0],[-1,27]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 22, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 23, 19, 20, 23,\n",
       "       13, 22, 21, 21, 21, 21, 21, 21, 21, 21, 22, 14, 10, 23, 19, 20, 22,\n",
       "       21, 22, 13, 22, 23, 14, 13, 24, 14,  7, 20, 21, 23, 19, 23, 17, 11,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 23, 21, 21, 21, 21, 21, 23, 20,\n",
       "       21, 22, 17, 24, 22, 20, 19, 21, 21, 21, 21, 21, 21, 21, 21, 21, 23,\n",
       "       19, 22, 21,  5,  4,  9, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 22, 21, 21, 21, 21, 23, 18,  8, 13, 19, 23, 22, 20, 21, 21, 21,\n",
       "       21, 23, 24, 24, 21, 21, 21, 21, 22, 21, 21, 22, 21, 23,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.reshape(test_np[1],[-1,27]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 23, 19, 20, 23,\n",
       "       13, 20, 21, 21, 21, 21, 21, 21, 21, 21, 20, 14, 10, 23, 19, 22, 22,\n",
       "       21, 22, 19, 20, 23, 14,  2, 14, 14,  7, 22, 21, 23, 19, 23, 17, 11,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 22, 21, 21, 21, 21, 21, 21,\n",
       "       21, 23, 17, 24, 22, 20, 19, 21, 21, 21, 21, 21, 21, 21, 22, 21, 23,\n",
       "       19, 22, 20, 15,  5,  9, 11, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21,\n",
       "       21, 22, 21, 21, 21, 21, 23, 14, 11, 13, 22, 21, 23, 19, 21, 21, 21,\n",
       "       21, 21, 21, 21, 21, 21, 21, 21, 22, 21, 21, 22, 22, 23,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(np.reshape(test_res[1],[-1,27]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encoder.predict(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_encoded[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3593759 ,  0.74483067],\n",
       "       [ 0.07032961, -0.8594195 ],\n",
       "       [ 0.30905998, -1.6250093 ],\n",
       "       ...,\n",
       "       [ 0.5164246 ,  1.1261108 ],\n",
       "       [ 1.3644242 ,  0.63759685],\n",
       "       [-0.8656409 , -0.5959314 ]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.122706 , -1.2144934],\n",
       "       [-1.9336755, -1.9306399],\n",
       "       [-1.0487787, -1.4388236],\n",
       "       ...,\n",
       "       [-2.0057704, -2.0212417],\n",
       "       [-1.7248856, -2.2287254],\n",
       "       [-1.6537818, -0.0442635]], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.122706 , -1.2144934],\n",
       "       [-1.9336755, -1.9306399],\n",
       "       [-1.0487787, -1.4388236],\n",
       "       ...,\n",
       "       [-2.0057704, -2.0212417],\n",
       "       [-1.7248856, -2.2287254],\n",
       "       [-1.6537818, -0.0442635]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_encoded[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dizy",
   "language": "python",
   "name": "dizy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
