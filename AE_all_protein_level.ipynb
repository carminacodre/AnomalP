{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE on proteins in SA representation - protein level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "from keras import Input\n",
    "from keras.layers import Dense, Lambda, Conv1D\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy, mse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import NotebookLoader\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "from tempfile import TemporaryFile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = {}\n",
    "for f in pre.families:\n",
    "    proteins = glob.glob(os.path.join(pre.family_paths[f], \"*.out\"))\n",
    "    print(\"Proteins for family %s\" %f)\n",
    "    for p in proteins:\n",
    "        print(p)\n",
    "    lengths[f] = len(p)\n",
    "total = sum([lengths[f] for f in pre.families])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if preprocessing should add padding or not \n",
    "# for fully convololutional networks it is not needed, otherwise must be set to True\n",
    "padding = True\n",
    "# if angle representation is used\n",
    "angles = False\n",
    "# length of alphabet used for encoding, 3 for angles\n",
    "num_classes = 25 if not angles else 3\n",
    "# categorical enconding of classes. for SA representation must be set to True, for angles False\n",
    "categorical = True if not angles else False\n",
    "# normalization of data \n",
    "normalize = False if not angles else True\n",
    "# max length of a sequence \n",
    "max_length = 668\n",
    "if angles:\n",
    "    max_length *= 3\n",
    "flatten = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "intermediate_dim = 10 if not angles else 25\n",
    "epochs = 20\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_set_for_family(f, set_type):\n",
    "    ds_path = os.path.join(ds_serialized_path, f, set_type)\n",
    "    files = glob.glob(os.path.join(ds_path, \"*.npy\"))\n",
    "    new = np.load(files[0])\n",
    "    for f in files[1:]:\n",
    "        conf_f = np.load(f)\n",
    "        new = np.concatenate([new,conf_f])\n",
    "        del conf_f\n",
    "    return new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_callbacks(f):\n",
    "    checkpoints_path = os.path.join(\"models_proteins\", f)\n",
    "    tensorboard_path = os.path.join(\"logs\", f)\n",
    "    cp_cb = ModelCheckpoint(filepath=os.path.join(checkpoints_path, \"model_protein_level_\" + f + \".hdf5\"), monitor='val_loss',\n",
    "                            save_best_only=True)\n",
    "    tb_cb = TensorBoard(log_dir=tensorboard_path)\n",
    "    return [cp_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder\n",
    "def get_ae():\n",
    "    if categorical:\n",
    "        if not flatten:\n",
    "            model_input = Input(shape=(None,num_classes))\n",
    "        else:\n",
    "            model_input = Input(shape=(max_length*num_classes,))\n",
    "    else:\n",
    "        model_input = Input(shape=(max_length,))\n",
    "    #x=Conv1D(intermediate_dim, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1)(model_input)\n",
    "    #encoded=Conv1D(intermediate_dim, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1, name=\"encoded\")(x)\n",
    "    #x=Conv1D(num_classes, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1)(encoded)\n",
    "    encoded= Dense(intermediate_dim, activation='sigmoid')(model_input)\n",
    "    if categorical:\n",
    "        if not flatten:\n",
    "            x = Dense(num_classes, activation='sigmoid')(encoded)\n",
    "        else:\n",
    "            x = Dense(max_length*num_classes, activation='sigmoid')(encoded)\n",
    "    else:\n",
    "        x = Dense(max_length, activation='sigmoid')(encoded)\n",
    "    ae=Model(inputs=model_input, outputs=[x])\n",
    "    opt=RMSprop(lr=learning_rate)\n",
    "    ae.compile(optimizer=opt, loss='binary_crossentropy', metrics=['mean_absolute_error'])\n",
    "    ae.summary()\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the autoencoder for specific classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"fam_1\": \"models_proteins/fam_1/model_protein_level_fam_1.hdf5\",\n",
    "          \"fam_2\": \"models_proteins/fam_2/model_protein_level_fam_2.hdf5\",\n",
    "          \"fam_3\": \"models_proteins/fam_3/model_protein_level_fam_3.hdf5\",\n",
    "          \"fam_4\": \"models_proteins/fam_4/model_protein_level_fam_4.hdf5\",\n",
    "          \"fam_5\": \"models_proteins/fam_5/model_protein_level_fam_5.hdf5\",\n",
    "          \"fam_6\": \"models_proteins/fam_6/model_protein_level_fam_6.hdf5\",\n",
    "          \"fam_7\": \"models_proteins/fam_7/model_protein_level_fam_7.hdf5\",\n",
    "          \"fam_8\": \"models_proteins/fam_8/model_protein_level_fam_8.hdf5\",\n",
    "          \"fam_9\": \"models_proteins/fam_9/model_protein_level_fam_9.hdf5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data_serialized_proteins_prot'\n",
    "#for r in range(1, 10):\n",
    "#    os.makedirs(os.path.join(path, 'fam_%d' % r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_fam(f):\n",
    "    print(\"Test for autoencoder on fam %s\" %f)\n",
    "    train = read_set_for_family(f,\"train\")\n",
    "    ae = load_model(models[f])\n",
    "    ae.summary()\n",
    "    losses_train = []\n",
    "    for t in train:\n",
    "        losses_train.append(ae.evaluate(np.array([t]),np.array([t]), verbose=0))\n",
    "    max_l = max(losses_train)\n",
    "    print(\"Max loss is %f\" %max_l)\n",
    "    losses_test = []\n",
    "    del train\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    tp_p, tn_p, fp_p, fn_p = 0, 0, 0, 0\n",
    "    for ft in pre.families:\n",
    "        print(\"Test for fam %s\" %ft)\n",
    "        ds_path = os.path.join(path, ft, \"test\")\n",
    "        files = glob.glob(os.path.join(ds_path, \"*.npy\"))\n",
    "        for file in files:\n",
    "            test = np.load(file)\n",
    "            total_nr = test.shape[0]\n",
    "            total_loss = 0.0\n",
    "            gt = 0\n",
    "            ls = 0\n",
    "            # check for each configuration the losses\n",
    "            for t in test:\n",
    "                loss=ae.evaluate(np.array([t]),np.array([t]), verbose=0)\n",
    "                total_loss +=loss\n",
    "                if loss > max_l:\n",
    "                    gt+=1\n",
    "                else:\n",
    "                    ls+=1\n",
    "            if gt >= ls:\n",
    "                # predict other family\n",
    "                if ft == f:\n",
    "                    fn+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "            else:\n",
    "                # predict current family\n",
    "                if ft == f:\n",
    "                    tp+=1 \n",
    "                else:\n",
    "                    fp+=1\n",
    "            # compute the probability\n",
    "            total_loss /= total_nr\n",
    "            if total_loss > max_l:\n",
    "                pr = 1 - max_l / (2 * total_loss)\n",
    "            else:\n",
    "                pr = total_loss / (2 * max_l)\n",
    "            if pr >= 0.5:\n",
    "                # predict other family\n",
    "                if ft == f:\n",
    "                    fn_p+=1\n",
    "                else:\n",
    "                    tn_p+=1\n",
    "            else:\n",
    "                # predict current family\n",
    "                if ft == f:\n",
    "                    tp_p+=1 \n",
    "                else:\n",
    "                    fp_p+=1                 \n",
    "    return [tp, tn, fp, fn], [tp_p, tn_p, fp_p, fn_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fam = open('res_fam_conf_prot.csv', mode='w')\n",
    "res_avg = open('res_avg_conf_prot.csv', mode='w')\n",
    "writer_fam = csv.writer(res_fam, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "writer_fam.writerow(['Iteration', 'Superfamily', 'TP', 'TN', 'FP', 'FN', 'Prec', 'Recall', 'Spec', 'AUC'])\n",
    "writer_avg = csv.writer(res_avg, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "writer_avg.writerow(['Iteration', 'Prec', 'Recall', 'Spec', 'AUC'])\n",
    "for i in range(0,20):\n",
    "    \n",
    "    # serialize data\n",
    "    for f in pre.families:\n",
    "        proteins = glob.glob(os.path.join(pre.family_paths[f], \"*.out\"))\n",
    "        for p in proteins:\n",
    "            print(p)\n",
    "            proteins_conf = []\n",
    "            with open(p) as in_file:\n",
    "                for line in in_file:\n",
    "                    proteins_conf.append(line.strip())\n",
    "            print(len(proteins_conf))\n",
    "            test_size = int(0.25 * len(proteins_conf))\n",
    "            val_size = int(0.15 * len(proteins_conf))\n",
    "            train_all_p, test_p = train_test_split(proteins_conf, test_size=test_size, random_state=i)\n",
    "            train_p, val_p = train_test_split(train_all_p, test_size = val_size, random_state=i)\n",
    "\n",
    "            #preprocess\n",
    "            train_p = pre.process_conf(train_p, categorical=categorical, use_angles=use_angles, padding=padding, max_length=max_length,normalize=normalize, flatten=flatten)\n",
    "            val_p = pre.process_conf(val_p, categorical=categorical, use_angles=use_angles, padding=padding, max_length=max_length,normalize=normalize, flatten=flatten)\n",
    "            test_p = pre.process_conf(test_p, categorical=categorical, use_angles=use_angles, padding=padding, max_length=max_length,normalize=normalize, flatten=flatten)\n",
    "            print(\"train: \" + repr(train_p.shape))\n",
    "            print(\"val: \" + repr(val_p.shape))\n",
    "            print(\"test: \" + repr(test_p.shape))\n",
    "            del train_all_p\n",
    "            del proteins_conf\n",
    "\n",
    "            p_name = os.path.basename(p).split('.')[0]\n",
    "            train_filename = os.path.join(path, f, \"train\", \"train_\"+ p_name +\".npy\")\n",
    "            val_filename = os.path.join(path, f, \"val\", \"val_\"+ p_name +\".npy\")\n",
    "            test_filename = os.path.join(path, f, \"test\", \"test_\"+ p_name +\".npy\")\n",
    "            np.save(train_filename, train_p)\n",
    "            np.save(val_filename, val_p)\n",
    "            np.save(test_filename, test_p)\n",
    "\n",
    "            del train_p\n",
    "            del test_p\n",
    "            del val_p\n",
    "            \n",
    "    # train autoencoders\n",
    "    for f in pre.families:\n",
    "        print(\"Training for family %s\" %f)\n",
    "        train = read_set_for_family(f,\"train\")\n",
    "        test = read_set_for_family(f,\"val\")\n",
    "        print(\"train: \" + repr(train.shape))\n",
    "        print(\"test\" + repr(test.shape))\n",
    "        ae = get_ae()\n",
    "        ae.fit(train, train,\n",
    "               shuffle=True,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               validation_data=(test, test),\n",
    "               callbacks=create_callbacks(f),\n",
    "               verbose=1)\n",
    "        del train\n",
    "        del test\n",
    "    \n",
    "    # evaluate autoencoders\n",
    "    tp, tn, fp, fn = {}, {}, {}, {}\n",
    "    tp_p, tn_p, fp_p, fn_p = {}, {}, {}, {}\n",
    "    prec, recall, spec, auc = {}, {}, {}, {}\n",
    "    prec_p, recall_p, spec_p, auc_p = {}, {}, {}, {}\n",
    "    for f in pre.families:\n",
    "        print(\"Evaluating family %s\" %f)\n",
    "        [tp[f], tn[f], fp[f], fn[f]], [tp_p[f], tn_p[f], fp_p[f], fn_p[f]] = evaluate_for_fam(f)\n",
    "        prec_p[f] = (1.0* tp_p[f] / (tp_p[f] + fp_p[f]))\n",
    "        recall_p[f] = (1.0* tp_p[f] / (tp_p[f] + fn_p[f]))\n",
    "        spec_p[f] = (1.0* tn_p[f] / (tn_p[f] + fp_p[f]))\n",
    "        auc_p[f] = (recall_p[f] + spec_p[f]) / 2\n",
    "        # write to csv \n",
    "        print([i, f, tp_p[f], tn_p[f], fp_p[f], tn_p[f], prec_p[f], recall_p[f], spec_p[f], auc_p[f]])\n",
    "        writer_fam.writerow([i, f, tp_p[f], tn_p[f], fp_p[f], tn_p[f], prec_p[f], recall_p[f], spec_p[f], auc_p[f]])\n",
    "    prec_wavg_p, recall_wavg_p, spec_wavg_p, auc_wavg_p = 0, 0, 0, 0\n",
    "    for f in pre.families:\n",
    "        prec_wavg_p += lengths[f] * prec_p[f] / total\n",
    "        recall_wavg_p += lengths[f] * recall_p[f] / total\n",
    "        spec_wavg_p += lengths[f] * spec_p[f] / total\n",
    "        auc_wavg_p += lengths[f] * auc_p[f] / total\n",
    "    # write to csv\n",
    "    print([i, prec_wavg_p, recall_wavg_p, spec_wavg_p, auc_wavg_p])\n",
    "    writer_avg.writerow([i, prec_wavg_p, recall_wavg_p, spec_wavg_p, auc_wavg_p])\n",
    "res_fam.close()\n",
    "res_avg.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dizy",
   "language": "python",
   "name": "dizy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
