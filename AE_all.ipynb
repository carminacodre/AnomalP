{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AE on proteins in SA representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from collections import Counter\n",
    "import string\n",
    "from keras import Input\n",
    "from keras.layers import Dense, Lambda, Conv1D\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.objectives import binary_crossentropy, mse\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import NotebookLoader\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.models import load_model\n",
    "from tempfile import TemporaryFile\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Preprocessing as pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lengths = {}\n",
    "for f in pre.families:\n",
    "    proteins = glob.glob(os.path.join(pre.family_paths[f], \"*.out\"))\n",
    "    print(\"Proteins for family %s\" %f)\n",
    "    for p in proteins:\n",
    "        print(p)\n",
    "    lengths[f] = len(p)\n",
    "total = sum([lengths[f] for f in pre.families])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding = True\n",
    "num_classes = 25\n",
    "categorical = True\n",
    "angles = False\n",
    "normalize = False\n",
    "max_length = 668\n",
    "flatten = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "intermediate_dim = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkpoints(f):\n",
    "    checkpoints_path = os.path.join(\"models\", f)\n",
    "    tensorboard_path = os.path.join(\"logs\", f)\n",
    "    cp_cb = ModelCheckpoint(filepath=os.path.join(checkpoints_path, \"model\" + f + \".hdf5\"), monitor='val_loss',\n",
    "                            save_best_only=True)\n",
    "    tb_cb = TensorBoard(log_dir=tensorboard_path)\n",
    "    return [cp_cb, tb_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#autoencoder\n",
    "def get_ae():\n",
    "    if categorical:\n",
    "        if not flatten:\n",
    "            model_input = Input(shape=(None,num_classes))\n",
    "        else:\n",
    "            model_input = Input(shape=(max_length*num_classes,))\n",
    "    else:\n",
    "        model_input = Input(shape=(max_length,))\n",
    "    #x=Conv1D(intermediate_dim, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1)(model_input)\n",
    "    #encoded=Conv1D(intermediate_dim, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1, name=\"encoded\")(x)\n",
    "    #x=Conv1D(num_classes, activation='sigmoid', kernel_size=3, padding='same', dilation_rate=1)(encoded)\n",
    "    encoded= Dense(intermediate_dim, activation='sigmoid')(model_input)\n",
    "    if categorical:\n",
    "        if not flatten:\n",
    "            x = Dense(num_classes, activation='sigmoid')(encoded)\n",
    "        else:\n",
    "            x = Dense(max_length*num_classes, activation='sigmoid')(encoded)\n",
    "    else:\n",
    "        x = Dense(max_length, activation='sigmoid')(encoded)\n",
    "    ae=Model(inputs=model_input, outputs=[x])\n",
    "    opt=RMSprop(lr=0.01)\n",
    "    ae.compile(optimizer=opt, loss='binary_crossentropy')\n",
    "    ae.summary()\n",
    "    return ae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the autoencoder for specific classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"fam_1\": \"models/fam_1/modelfam_1.hdf5\",\n",
    "          \"fam_2\": \"models/fam_2/modelfam_2.hdf5\",\n",
    "          \"fam_3\": \"models/fam_3/modelfam_3.hdf5\",\n",
    "          \"fam_4\": \"models/fam_4/modelfam_4.hdf5\",\n",
    "          \"fam_5\": \"models/fam_5/modelfam_5.hdf5\",\n",
    "          \"fam_6\": \"models/fam_6/modelfam_6.hdf5\",\n",
    "          \"fam_7\": \"models/fam_7/modelfam_7.hdf5\",\n",
    "          \"fam_8\": \"models/fam_8/modelfam_8.hdf5\",\n",
    "          \"fam_9\": \"models/fam_9/modelfam_9.hdf5\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_serialized_path = \"data_serialized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_fam(f):\n",
    "    print(\"Test for autoencoder on fam %s\" %f)\n",
    "    train_filename = os.path.join(ds_serialized_path, f, \"train.npy\")\n",
    "    train = np.load(train_filename)\n",
    "    ae = load_model(models[f])\n",
    "    ae.summary()\n",
    "    losses_train = []\n",
    "    for t in train:\n",
    "        losses_train.append(ae.evaluate(np.array([t]),np.array([t]), verbose=0))\n",
    "    max_l = max(losses_train)\n",
    "    losses_test = []\n",
    "    del train\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "    tp_p, tn_p, fp_p, fn_p = 0, 0, 0, 0\n",
    "    for ft in families:\n",
    "        print(\"Test for fam %s\" %ft)\n",
    "        test_filename = os.path.join(ds_serialized_path, ft, \"test.npy\")\n",
    "        test = np.load(test_filename)\n",
    "        for t in test:\n",
    "            loss=ae.evaluate(np.array([t]),np.array([t]), verbose=0)\n",
    "            if loss > max_l:\n",
    "                # predict other family\n",
    "                if ft == f:\n",
    "                    fn+=1\n",
    "                else:\n",
    "                    tn+=1\n",
    "            else:\n",
    "                # predict current family\n",
    "                if ft == f:\n",
    "                    tp+=1 \n",
    "                else:\n",
    "                    fp+=1\n",
    "            # compute the probability\n",
    "            if loss > max_l:\n",
    "                pr = 1 - (max_l / (2 * loss))\n",
    "            else:\n",
    "                pr = loss / (2 * max_l)\n",
    "            if pr >= 0.5:\n",
    "                # predict other family\n",
    "                if ft == f:\n",
    "                    fn_p+=1\n",
    "                else:\n",
    "                    tn_p+=1\n",
    "            else:\n",
    "                # predict current family\n",
    "                if ft == f:\n",
    "                    tp_p+=1 \n",
    "                else:\n",
    "                    fp_p+=1  \n",
    "    return [tp, tn, fp, fn], [tp_p, tn_p, fp_p, fn_p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_fam = open('res_fam_conf.csv', mode='w')\n",
    "res_avg = open('res_avg_conf.csv', mode='w')\n",
    "writer_fam = csv.writer(res_fam, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "writer_fam.writerow(['Iteration', 'Superfamily', 'TP', 'TN', 'FP', 'FN', 'Prec', 'Recall', 'Spec', 'AUC'])\n",
    "writer_avg = csv.writer(res_avg, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "writer_avg.writerow(['Iteration', 'Prec', 'Recall', 'Spec', 'AUC'])\n",
    "for i in range(0,20):\n",
    "    for f in pre.families:\n",
    "        families_conf = pre.load_family(f)\n",
    "        families_conf = pre.process_conf(families_conf, categorical=categorical, angles=angles, padding=padding, max_length=max_length,normalize=normalize, flatten=flatten)  \n",
    "        test_size = int(0.25 * families_conf.shape[0])\n",
    "        val_size = int(0.15 * families_conf.shape[0])\n",
    "        train_all, test = train_test_split(families_conf, test_size=test_size, random_state=i)\n",
    "        train, val = train_test_split(train_all, test_size = val_size, random_state=i)\n",
    "        del families_conf\n",
    "        del train_all\n",
    "        print(\"train: \" + repr(train.shape))\n",
    "        print(\"val: \" + repr(val.shape))\n",
    "        print(\"test: \" + repr(test.shape))\n",
    "        train_filename = os.path.join(\"data_serialized\", f, \"train.npy\")\n",
    "        val_filename = os.path.join(\"data_serialized\", f, \"val.npy\")\n",
    "        test_filename = os.path.join(\"data_serialized\", f, \"test.npy\")\n",
    "        np.save(train_filename, train)\n",
    "        np.save(val_filename, val)\n",
    "        np.save(test_filename, test)\n",
    "        del train\n",
    "        del test\n",
    "        del val\n",
    "    for f in pre.families:\n",
    "        print(\"Training for family %s\" %f)\n",
    "        train_filename = os.path.join(\"data_serialized\", f, \"train.npy\")\n",
    "        test_filename = os.path.join(\"data_serialized\", f, \"val.npy\")\n",
    "        train = np.load(train_filename)\n",
    "        test = np.load(test_filename)\n",
    "        print(\"train: \" + repr(train.shape))\n",
    "        print(\"test\" + repr(test.shape))\n",
    "        ae = get_ae()\n",
    "        ae.fit(train, train,\n",
    "               shuffle=True,\n",
    "               epochs=epochs,\n",
    "               batch_size=batch_size,\n",
    "               validation_data=(test, test),\n",
    "               callbacks=create_checkpoints(f),\n",
    "               verbose=1)    \n",
    "    # evaluate\n",
    "    tp, tn, fp, fn = {}, {}, {}, {}\n",
    "    tp_p, tn_p, fp_p, fn_p = {}, {}, {}, {}\n",
    "    prec, recall, spec, auc = {}, {}, {}, {}\n",
    "    prec_p, recall_p, spec_p, auc_p = {}, {}, {}, {}\n",
    "    for f in pre.families:\n",
    "        print(\"Evaluating family %s\" %f)\n",
    "        [tp[f], tn[f], fp[f], fn[f]], [tp_p[f], tn_p[f], fp_p[f], fn_p[f]] = evaluate_for_fam(f)\n",
    "        prec_p[f] = (1.0* tp_p[f] / (tp_p[f] + fp_p[f]))\n",
    "        recall_p[f] = (1.0* tp_p[f] / (tp_p[f] + fn_p[f]))\n",
    "        spec_p[f] = (1.0* tn_p[f] / (tn_p[f] + fp_p[f]))\n",
    "        auc_p[f] = (recall_p[f] + spec_p[f]) / 2\n",
    "        # write to csv \n",
    "        writer_fam.writerow([i, f, tp_p[f], tn_p[f], fp_p[f], tn_p[f], prec_p[f], recall_p[f], spec_p[f], auc_p[f]])\n",
    "    prec_wavg_p, recall_wavg_p, spec_wavg_p, auc_wavg_p = 0, 0, 0, 0\n",
    "\n",
    "    for f in pre.families:\n",
    "        prec_wavg_p += lengths[f] * prec_p[f] / total\n",
    "        recall_wavg_p += lengths[f] * recall_p[f] / total\n",
    "        spec_wavg_p += lengths[f] * spec_p[f] / total\n",
    "        auc_wavg_p += lengths[f] * auc_p[f] / total\n",
    "        # write to csv\n",
    "        writer_avg.writerow([i, prec_wavg_p, recall_wavg_p, spec_wavg_p, auc_wavg_p])\n",
    "res_fam.close()\n",
    "res_avg.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sn\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.figure(1, figsize=(15,15))\n",
    "# for i,f in enumerate(families):\n",
    "#     df_cm = pd.DataFrame([[tn[f], fp[f]],[ fn[f], tp[f]]], index = [\"not \" + f, f],\n",
    "#                       columns = [\"not \" + f, f])\n",
    "#     plt.subplot(330 + i + 1)\n",
    "#     sn.heatmap(df_cm, annot=True, fmt='g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dizy",
   "language": "python",
   "name": "dizy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
